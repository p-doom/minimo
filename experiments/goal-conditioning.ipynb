{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal-oriented self-improvement for Automated Theorem Proving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method\n",
    "We are basically thinking of extending minimo by introducing a goal i.e. a set of problems the model should eventually solve. We do this by adding loss-term forcing the model to sample conjectures from our goal set. We control this ‘forcing’ using a hyperparameter `alpha`. A higher value gives the `progess_loss` more value, thus pushes the model towards the goal stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environment\t  launch    pyproject.toml\t     slurm-1275910.out\n",
      "experiments\t  learning  README.md\t\t     tutorial.md\n",
      "FAQ.md\t\t  LICENSE   redis_hostname_port.txt  wandb\n",
      "goals\t\t  logs\t    setup.sh\n",
      "install_redis.sh  outputs   slurm-1275909.out\n"
     ]
    }
   ],
   "source": [
    "# lets move to the parent directory so it is easier to run the scripts\n",
    "import os\n",
    "\n",
    "if os.getcwd().split('/')[-1] == 'experiments':\n",
    "    os.chdir('../')\n",
    "\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start some workers so our tasks get executed in parallel. \n",
    "# @franz you would need to start them on the same node\n",
    "sbatch --job-name=redis --cpus-per-task=10 --mem=50G --time=5:00:00 --wrap=\"./launch/start_redis.sh\"\n",
    "\n",
    "sbatch --job-name=worker1 --cpus-per-task=4 --gres=gpu:1 --mem=50G --time=5:00:00 --wrap=\"./launch/start_worker.sh\"\n",
    "sbatch --job-name=worker2 --cpus-per-task=4 --gres=gpu:1 --mem=50G --time=5:00:00 --wrap=\"./launch/start_worker.sh\"\n",
    "sbatch --job-name=worker3 --cpus-per-task=4 --gres=gpu:1 --mem=50G --time=5:00:00 --wrap=\"./launch/start_worker.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the now we use a very simple goal set. It consists of a single goal which is proving the theorem: \n",
    "\n",
    "`a is a natural number: (0 + a) = a`\n",
    "\n",
    "Or in peano:\n",
    "\n",
    "`[('a0 : nat) -> (= (+ z 'a0) 'a0)]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Overfit on the goal set\n",
    "After implementing the goal-conditioning, let's run experiments with `alpha=0` and `alpha=1`. We expect the `progress_loss` to go down i.e. the model overfits to the `final_goal` set. However, the actual `train_loss` shouldn’t go down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275940\n",
      "Submitted batch job 1275941\n"
     ]
    }
   ],
   "source": [
    "# start a job with alpha=1 to overfit on the goals\n",
    "!sbatch --job-name=train_alpha_1 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=1:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.max_mcts_nodes=100 agent.policy.alpha=1 agent.policy.total_iterations=2\"\n",
    "\n",
    "# start a job with alpha=0 as a baseline\n",
    "!sbatch --job-name=train_alpha_0 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=1:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.max_mcts_nodes=100 agent.policy.alpha=0 agent.policy.total_iterations=2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works! The ``training_loss`` diverges for `alpha=1`. The `progress_loss` struggles to go below a certain threshold for `alpha=0`. As per our intuition, the former overfits to sampling the final theorem and can't solve any conjectures in the second iteration. We need something better for alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Try different values for alpha\n",
    "\n",
    "We try different schedules for alpha in the hope that the training loss and progress loss both converge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275099\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_0_8 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.8\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_6 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.6\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.4\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_2 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.2\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_3e_4 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=3e-4\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_0_1e_3 --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=24:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1e-3\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works as well. We also observe a correlation between alpha values and the ratio of conjectures that the model is able to prove per iteration. The higher the alpha value, the fewer problems the model can solve. What if we could let the model explore problems for itself for a while and at a later stage push it towards our goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Alpha Schedules\n",
    "\n",
    "We implement fancy schedules to 'warm-up' alpha over several iterations. The options are \n",
    "\n",
    "`alpha_schedule = [ constant | linear | quadratic | cubic | cos ]`\n",
    "\n",
    "#### 3.1 Warm up alpha to 1.0\n",
    "\n",
    "As a first step we warm up alpha to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1274969\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_lin --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=linear\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_quad --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=quadratic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=cubic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cos --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=cos\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is apparent that high values of alpha just overpower the actual loss. Ideally we wan't to keep alpha much smaller. Let's try just warming it up to lower values. \n",
    "\n",
    "#### 3.2 Warm up alpha to lower values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1274557\n",
      "Submitted batch job 1274558\n",
      "Submitted batch job 1274559\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.2 alpha_schedule=cubic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.3 alpha_schedule=cubic\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_cubic --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.4 alpha_schedule=cubic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Use the ratio of solved conjectures \n",
    "\n",
    "Now comes the actually interesting part. We implement a more principled schedule.\n",
    "Let's use the ratio of solved conjectures to total sampled conjectures to directly control alpha. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1274560\n",
      "Submitted batch job 1274561\n"
     ]
    }
   ],
   "source": [
    "# TODO this still runs on single GPU\n",
    "!sbatch --job-name=train_alpha_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=ratio\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0.2 alpha_schedule=ratio\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275950\n",
      "Submitted batch job 1275951\n"
     ]
    }
   ],
   "source": [
    "# start a job with alpha schedule ratio and max_alpha=1\n",
    "!sbatch --job-name=train_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.policy.alpha=1 agent.policy.alpha_schedule=ratio goals=nat-add-hard\"\n",
    "\n",
    "# start a job with alpha schedule ratio and max_alpha=5e-3\n",
    "!sbatch --job-name=train_ratio_small --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.policy.alpha=5e-3 agent.policy.alpha_schedule=ratio goals=nat-add-hard\"\n",
    "\n",
    "# start a job with alpha=0 as a baseline\n",
    "# !sbatch --job-name=train_vanilla --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=10:00:00 --wrap=\"./launch/run_bootstrap_distributed.sh agent.policy.alpha=0 goals=nat-add-hard\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Increase max iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 1275039\n",
      "Submitted batch job 1275040\n",
      "Submitted batch job 1275041\n"
     ]
    }
   ],
   "source": [
    "!sbatch --job-name=train_alpha_long --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=20:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=0 iterations=30\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_long_ratio --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=20:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=ratio iterations=30\"\n",
    "\n",
    "!sbatch --job-name=train_alpha_long_cos --cpus-per-task=4 --mem=50G --gres=gpu:1,VRAM=12G --time=20:00:00 --wrap=\"python learning/bootstrap.py theory=nat-add alpha=1 alpha_schedule=cubic iterations=30\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "minimo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
